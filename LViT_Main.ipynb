{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/HUANGLIZI/LViT.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dEynuYkGSBRO",
        "outputId": "58a61a4f-5dac-4196-beb3-8edd5179a244"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'LViT'...\n",
            "remote: Enumerating objects: 323, done.\u001b[K\n",
            "remote: Counting objects: 100% (70/70), done.\u001b[K\n",
            "remote: Compressing objects: 100% (9/9), done.\u001b[K\n",
            "remote: Total 323 (delta 65), reused 61 (delta 61), pack-reused 253 (from 1)\u001b[K\n",
            "Receiving objects: 100% (323/323), 92.10 MiB | 29.88 MiB/s, done.\n",
            "Resolving deltas: 100% (123/123), done.\n",
            "Updating files: 100% (121/121), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ml_collections\n"
      ],
      "metadata": {
        "id": "36zGM45w8xIi",
        "outputId": "09fe53e7-c846-4f29-fc43-2a7f33f125de",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: ml_collections in /usr/local/lib/python3.11/dist-packages (1.1.0)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from ml_collections) (1.4.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from ml_collections) (6.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd LViT\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PuFxibYjSOB5",
        "outputId": "8c584233-7dd5-4b5f-b6c5-4da0090711d4"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/LViT\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rgJe-KrQSVEf",
        "outputId": "4376052a-f8c3-49ec-9a96-061ca32c6e8f"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/LViT/datasets\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U-LOsoikUKW5",
        "outputId": "3b7eec72-95bb-4559-ce7c-c077ac52cb50"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import time\n",
        "import ml_collections\n",
        "\n",
        "# Set CUDA and random seed\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
        "use_cuda = torch.cuda.is_available()\n",
        "seed = 666\n",
        "os.environ['PYTHONHASHSEED'] = str(seed)\n"
      ],
      "metadata": {
        "id": "NKf8irBo9cNc"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LEKAICfDiKNH"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "PuV2W8Wo869u"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "class Config:\n",
        "    task_name = 'MoNuSeg'\n",
        "    model_name = 'LViT'  # or 'LViT_pretrain' if using a pretrained model\n",
        "    img_size = 224\n",
        "    batch_size = 4\n",
        "    epochs = 100\n",
        "    learning_rate = 1e-4\n",
        "    seed = 42\n",
        "    cosineLR = True\n",
        "    early_stopping_patience = 10\n",
        "    session_name = 'MoNuSeg_LViT_Session'\n",
        "    save_path = './models/'\n",
        "    tensorboard_folder = './tensorboard_logs/'\n",
        "    logger_path = './training.log'\n",
        "    train_dataset = './datasets/MoNuSeg/Train_Folder/'\n",
        "    val_dataset = './datasets/MoNuSeg/Val_Folder/'\n",
        "    test_dataset = './datasets/MoNuSeg/Test_Folder/'\n",
        "    n_channels = 3\n",
        "    n_labels = 1  # Adjust based on your dataset\n",
        "\n",
        "config = Config()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import random\n",
        "from scipy.ndimage.interpolation import zoom\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import transforms as T\n",
        "from torchvision.transforms import functional as F\n",
        "import os\n",
        "import cv2\n",
        "from scipy import ndimage\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g5SL2qDAUNNC",
        "outputId": "0a1f0075-08d0-42f0-9328-7f35ac60e5ad"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-29-4647f5080857>:4: DeprecationWarning: Please import `zoom` from the `scipy.ndimage` namespace; the `scipy.ndimage.interpolation` namespace is deprecated and will be removed in SciPy 2.0.0.\n",
            "  from scipy.ndimage.interpolation import zoom\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "bert_model = AutoModel.from_pretrained(\"bert-base-uncased\")\n",
        "bert_model.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SzasBh4jWW7F",
        "outputId": "455ed059-cbf2-42e3-fadd-e00f2f999634"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertModel(\n",
              "  (embeddings): BertEmbeddings(\n",
              "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "    (position_embeddings): Embedding(512, 768)\n",
              "    (token_type_embeddings): Embedding(2, 768)\n",
              "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (encoder): BertEncoder(\n",
              "    (layer): ModuleList(\n",
              "      (0-11): 12 x BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSdpaSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (intermediate_act_fn): GELUActivation()\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (pooler): BertPooler(\n",
              "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "    (activation): Tanh()\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def random_rot_flip(image, label):\n",
        "    k = np.random.randint(0, 4)\n",
        "    image = np.rot90(image, k)\n",
        "    label = np.rot90(label, k)\n",
        "    axis = np.random.randint(0, 2)\n",
        "    image = np.flip(image, axis=axis).copy()\n",
        "    label = np.flip(label, axis=axis).copy()\n",
        "    return image, label\n",
        "\n",
        "def random_rotate(image, label):\n",
        "    angle = np.random.randint(-20, 20)\n",
        "    image = ndimage.rotate(image, angle, order=0, reshape=False)\n",
        "    label = ndimage.rotate(label, angle, order=0, reshape=False)\n",
        "    return image, label\n",
        "\n",
        "def to_long_tensor(pic):\n",
        "    img = torch.from_numpy(np.array(pic, np.uint8))\n",
        "    return img.long()\n",
        "\n",
        "def correct_dims(*images):\n",
        "    corr_images = []\n",
        "    for img in images:\n",
        "        if len(img.shape) == 2:\n",
        "            corr_images.append(np.expand_dims(img, axis=2))\n",
        "        else:\n",
        "            corr_images.append(img)\n",
        "    return corr_images if len(corr_images) > 1 else corr_images[0]"
      ],
      "metadata": {
        "id": "swkmRY6hWnKp"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RandomGenerator(object):\n",
        "    def __init__(self, output_size):\n",
        "        self.output_size = output_size\n",
        "\n",
        "    def __call__(self, sample):\n",
        "        image, label, text = sample['image'], sample['label'], sample['text']\n",
        "        image, label = image.astype(np.uint8), label.astype(np.uint8)\n",
        "        image, label = F.to_pil_image(image), F.to_pil_image(label)\n",
        "        x, y = image.size\n",
        "        if random.random() > 0.5:\n",
        "            image, label = random_rot_flip(image, label)\n",
        "        elif random.random() > 0.5:\n",
        "            image, label = random_rotate(image, label)\n",
        "        if x != self.output_size[0] or y != self.output_size[1]:\n",
        "            image = zoom(image, (self.output_size[0] / x, self.output_size[1] / y), order=3)\n",
        "            label = zoom(label, (self.output_size[0] / x, self.output_size[1] / y), order=0)\n",
        "        image = F.to_tensor(image)\n",
        "        label = to_long_tensor(label)\n",
        "        text = torch.Tensor(text)\n",
        "        return {'image': image, 'label': label, 'text': text}\n"
      ],
      "metadata": {
        "id": "Jzcfs6eK8-sY"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Callable\n"
      ],
      "metadata": {
        "id": "_eqjHkzoXUXy"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ImageToImage2D(Dataset):\n",
        "\n",
        "    def __init__(self, dataset_path: str, task_name: str, row_text: dict, joint_transform: Callable = None,\n",
        "                 one_hot_mask: int = False, image_size: int = 224) -> None:\n",
        "        self.dataset_path = dataset_path\n",
        "        self.image_size = image_size\n",
        "        self.input_path = os.path.join(dataset_path, 'img')\n",
        "        self.output_path = os.path.join(dataset_path, 'labelcol')\n",
        "        self.images_list = os.listdir(self.input_path)\n",
        "        self.mask_list = os.listdir(self.output_path)\n",
        "        self.one_hot_mask = one_hot_mask\n",
        "        self.rowtext = row_text\n",
        "        self.task_name = task_name\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "        self.bert_model = AutoModel.from_pretrained(\"bert-base-uncased\")\n",
        "        self.bert_model.eval()\n",
        "\n",
        "        if joint_transform:\n",
        "            self.joint_transform = joint_transform\n",
        "        else:\n",
        "            to_tensor = T.ToTensor()\n",
        "            self.joint_transform = lambda x, y: (to_tensor(x), to_tensor(y))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(os.listdir(self.input_path))\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image_filename = self.images_list[idx]\n",
        "        mask_filename = image_filename[: -3] + \"png\"\n",
        "\n",
        "        image = cv2.imread(os.path.join(self.input_path, image_filename))\n",
        "        image = cv2.resize(image, (self.image_size, self.image_size))\n",
        "\n",
        "        mask = cv2.imread(os.path.join(self.output_path, mask_filename), 0)\n",
        "        mask = cv2.resize(mask, (self.image_size, self.image_size))\n",
        "        mask[mask <= 0] = 0\n",
        "        mask[mask > 0] = 1\n",
        "\n",
        "        image, mask = correct_dims(image, mask)\n",
        "\n",
        "        text = self.rowtext[mask_filename]\n",
        "        text = text.split('\\n')\n",
        "        text = ' '.join(text)  # merge multi-line input if needed\n",
        "        inputs = self.tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
        "        with torch.no_grad():\n",
        "           outputs = self.bert_model(**inputs)\n",
        "        text_embedding = outputs.last_hidden_state.squeeze(0)  # shape: (seq_len, hidden_size)\n",
        "        if text_embedding.shape[0] > 10:\n",
        "             text_embedding = text_embedding[:10, :]\n",
        "        text = text_embedding.cpu().numpy()\n",
        "\n",
        "\n",
        "        if self.one_hot_mask:\n",
        "            assert self.one_hot_mask > 0, 'one_hot_mask must be nonnegative'\n",
        "            mask = torch.zeros((self.one_hot_mask, mask.shape[1], mask.shape[2])).scatter_(0, mask.long(), 1)\n",
        "\n",
        "        sample = {'image': image, 'label': mask, 'text': text}\n",
        "        if self.joint_transform:\n",
        "            sample = self.joint_transform(sample)\n",
        "\n",
        "        return sample, image_filename\n"
      ],
      "metadata": {
        "id": "ND1pxzJaWzrx"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import transforms as T\n",
        "from PIL import Image\n",
        "\n",
        "class CustomLV2D(Dataset):\n",
        "    def __init__(self, image_dir, label_dir, excel_path, image_size=224, one_hot_mask=True):\n",
        "        self.image_dir = image_dir\n",
        "        self.label_dir = label_dir\n",
        "        self.image_size = image_size\n",
        "        self.one_hot_mask = one_hot_mask\n",
        "\n",
        "        # Load Excel metadata\n",
        "        df = pd.read_excel(excel_path)\n",
        "\n",
        "        # Dynamically find columns (fallbacks if names vary)\n",
        "        self.image_col = next((col for col in df.columns if 'image' in col.lower()), df.columns[0])\n",
        "        self.text_col = next((col for col in df.columns if 'text' in col.lower()), df.columns[1])\n",
        "\n",
        "        self.image_names = df[self.image_col].astype(str).tolist()\n",
        "        self.texts = df[self.text_col].astype(str).tolist()\n",
        "\n",
        "        # Image transform\n",
        "        self.transform_img = T.Compose([\n",
        "            T.Resize((self.image_size, self.image_size)),\n",
        "            T.ToTensor()\n",
        "        ])\n",
        "\n",
        "        self.transform_mask = T.Compose([\n",
        "            T.Resize((self.image_size, self.image_size), interpolation=T.InterpolationMode.NEAREST)\n",
        "        ])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_names)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = self.image_names[idx]\n",
        "\n",
        "        # Normalize image and label filenames\n",
        "        img_path = os.path.join(self.image_dir, img_name)\n",
        "        img_path = img_path.replace('.jpeg', '.jpg')  # Optional: normalize extensions\n",
        "\n",
        "        label_name = img_name.rsplit('.', 1)[0] + '.png'  # Replace extension with .png\n",
        "        label_path = os.path.join(self.label_dir, label_name)\n",
        "\n",
        "        # Load image\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "        image = self.transform_img(image)\n",
        "\n",
        "        # Load and binarize mask\n",
        "        label = Image.open(label_path).convert(\"L\")\n",
        "        label = self.transform_mask(label)\n",
        "        label = np.array(label)\n",
        "        label = (label > 127).astype(np.uint8)  # binarize\n",
        "\n",
        "        if self.one_hot_mask:\n",
        "            one_hot = np.zeros((2, label.shape[0], label.shape[1]), dtype=np.float32)\n",
        "            one_hot[0] = (label == 0)\n",
        "            one_hot[1] = (label == 1)\n",
        "            label = torch.from_numpy(one_hot)\n",
        "        else:\n",
        "            label = torch.tensor(label, dtype=torch.long).unsqueeze(0)  # Add channel dim\n",
        "\n",
        "        text = self.texts[idx]\n",
        "\n",
        "        return {\n",
        "            'image': image,\n",
        "            'label': label,\n",
        "            'text': text\n",
        "        }\n"
      ],
      "metadata": {
        "id": "B5mkq3j0XW-U"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = CustomLV2D(\n",
        "    image_dir='/content/LViT/datasets/MoNuSeg/Train_Folder/img',\n",
        "    label_dir='/content/LViT/datasets/MoNuSeg/Train_Folder/labelcol',\n",
        "    excel_path='/content/LViT/datasets/MoNuSeg/Train_Folder/Train_text.xlsx',\n",
        "    image_size=224,\n",
        "    one_hot_mask=True\n",
        ")\n",
        "\n",
        "# Check one sample\n",
        "sample = train_dataset[0]\n",
        "print(sample['image'].shape)  # [3, 224, 224]\n",
        "print(sample['label'].shape)  # [2, 224, 224]\n",
        "print(sample['text'])         # description string\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ih1HAIxpOe1T",
        "outputId": "1ba5c517-3003-4f15-dd3f-a843b4526755"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([3, 224, 224])\n",
            "torch.Size([2, 224, 224])\n",
            "The nuclei are evenly distributed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_excel('/content/LViT/datasets/MoNuSeg/Train_Folder/Train_text.xlsx')\n",
        "print(df.columns)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WwZyZ33CrExK",
        "outputId": "362b66a2-e125-4a8f-c716-80106c8c1cf2"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['Image', 'Description'], dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Cb38wvbXrlYS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}